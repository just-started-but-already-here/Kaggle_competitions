{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%markdown\n# **Titanic notebook** <br> \nTitanic competition in Kaggle *https://www.kaggle.com/c/titanic/*<br>\n## **Overview:**\n### *I- Descriptive analysis and Data visualization*\n### *II- Data preprocessing*\n### *III- Machine Learning algorithms with sklearn and xgboost*\n### *IV- Deep Learning Classification with Pytorch*\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Imports\n## Utils\nimport pandas as pd #For dataframes manipulations\nimport numpy as np \nfrom IPython.display import display\nfrom collections import namedtuple\n\n##Data visualizations\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n#########################\n\n##Machine learning imports\n#Preprocessing\nfrom sklearn.preprocessing import LabelEncoder # label encoder for our categorical data\nfrom sklearn.model_selection import train_test_split # Data splitting into train and validation sets \n#Classifiers\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression \nfrom sklearn.ensemble import RandomForestClassifier # Random Forest \nfrom sklearn.tree import DecisionTreeClassifier # Decision tree\nimport xgboost # Xgboost  \n#Accuracy Metric \nfrom sklearn.metrics import accuracy_score # To calculate accuracies\n\n#########################\n\n##Deep learning imports (PyTorch)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import our datasets : training and testing\ninput_path = '/kaggle/input/titanic/'\ntrain_set = pd.read_csv(input_path+'train.csv')\ntest_set = pd.read_csv(input_path+'test.csv')\ndataset = [train_set, test_set]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%markdown\n## **Descriptive analysis and Data visualization**","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Columns of training dataset\")\nprint(train_set.columns.values)\nprint(\"----------------------------------------------\\n\\n\")\nprint(\"Training dataset info\")\ndisplay(train_set.info())\nprint(\"----------------------------------------------\\n\\n\")\nprint(\"training dataset description\")\ndisplay(train_set.describe())\nprint(\"----------------------------------------------\\n\\n\")\nprint(\"training dataset description of columns of type object\")\ndisplay(train_set.describe(include=['object'])) #description of categorical columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"\nthe commented lines are another way to visualise the data which bigger plots \n\"\"\"\n##Survived by Pclass\n#plt.figure(figsize=(30,20))\n#plt.subplot(2, 2, 1)\nplt.figure(figsize=(25,10))\nplt.subplot(2, 3, 1)\nax = sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_set).set_title('Ticket class')\n\n##Survived by Sex\n#plt.subplot(2, 2, 2)\nplt.subplot(2, 3, 2)\nsns.countplot(train_set['Sex'], hue=\"Survived\", data=train_set).set_title('Sex')\n#plt.show()\n\n##Survived by SibSp\n#plt.figure(figsize=(30,20))\n#plt.subplot(2, 2, 1)\nplt.subplot(2, 3, 3)\nsns.countplot(train_set['SibSp'], hue=\"Survived\", data=train_set).set_title('Number of siblings / spouses aboard the Titanic')\n\n##Survived by Parch\n#plt.subplot(2, 2, 2)\nplt.subplot(2, 3, 4)\nsns.countplot(train_set['Parch'], hue=\"Survived\", data=train_set).set_title('Number of parents / children aboard the Titanic')\n#plt.show()\n\n##Survived or not\n#plt.figure(figsize=(30,20))\n#plt.subplot(2, 2, 1)\nplt.subplot(2, 3, 5)\nsns.countplot(train_set['Survived']).set_title('Survival')\n\n##Survived by Embarked\n#plt.subplot(2, 2, 2)\nplt.subplot(2, 3, 6)\nsns.countplot(train_set['Embarked'], hue=\"Survived\", data=train_set).set_title('Port of Embarkation')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"g = sns.FacetGrid(train_set,height=4, col=\"Sex\", row=\"Survived\", margin_titles=True, hue = \"Survived\")\ng = g.map(plt.hist, \"Age\", edgecolor = 'white', bins = 8)\ng.fig.suptitle(\"Survived by Sex and Age\", size = 30)\nplt.subplots_adjust(top=0.85)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"g = sns.FacetGrid(train_set,height=4, col=\"Pclass\", row=\"Survived\", margin_titles=True, hue = \"Survived\")\ng = g.map(plt.hist, \"Age\", edgecolor = 'white', bins = 8)\ng.fig.suptitle(\"Survived by Ticket class and Age\", size = 35)\nplt.subplots_adjust(top=0.85)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#missing values\ndef missing_values_df(df):\n    \"\"\"\n    :input df: (Dataframe) input dataframe \n    :output output_df: (Dataframe) dataframe of missing values and there percentage \n    \"\"\"\n    missing_values = df.isnull().sum().sort_values(ascending = False)\n    missing_values = missing_values[missing_values>0]\n    ratio = missing_values/len(df)*100\n    output_df= pd.concat([missing_values, ratio], axis=1, keys=['Total missing values', 'Percentage'])\n    return output_df\nprint('Missing values in the columns of training dataset with percentage')\ndisplay(missing_values_df(train_set))\nprint('\\n------------------------------------------------\\n')\nprint('Missing values in the columns of test dataset with percentage')\ndisplay(missing_values_df(test_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown\n## **Data Preprocessing**\n1) fill the *NaN* values with mean (*Age feature*) our more frequent values (*Embarked feature*) <br>\n2) Add *titles* of passengers from names then delete names <br>\n3) Encode categorical features into integers <br>\n4) make 4 bins of age to categorize it <br>\n5) Normalize our datasets(training_set and test_set)<br>\n6) Split our training set into training and validation sets (80%/20%) <br>","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in range(len(dataset)):\n    freq_port = dataset[i]['Embarked'].dropna().mode()[0]\n    dataset[i]['Embarked'] = dataset[i]['Embarked'].fillna(freq_port)\n    dataset[i] = dataset[i].fillna(dataset[i].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Titels of passengers by sex\")\ndataset[0]['Title'] = dataset[0].Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndisplay(pd.crosstab(dataset[0]['Sex'], dataset[0]['Title']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i, data in enumerate(dataset):\n    dataset[i]['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    #pd.crosstab(dataset[0]['Title'], dataset[0]['Sex'])\n    dataset[i]['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', \n                                                 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset[i]['Title'] = data['Title'].replace('Mlle', 'Miss')\n    dataset[i]['Title'] = data['Title'].replace('Ms', 'Miss')\n    dataset[i]['Title'] = data['Title'].replace('Mme', 'Mrs')\n    print(dataset[i]['Title'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"encoder=LabelEncoder()\ncategoricalFeatures = dataset[0].select_dtypes(include=['object']).columns\nfor i, data in enumerate(dataset):\n    data[categoricalFeatures]=data[categoricalFeatures].astype(str)\n    encoded = data[categoricalFeatures].apply(encoder.fit_transform)\n    for j in categoricalFeatures:\n        dataset[i][j]=encoded[j]\ndataset[0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"bins = [0,18,60,80]\nlabels = [1,2,3]\nfor i, data in enumerate(dataset):\n    dataset[i] = dataset[i].drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n    dataset[i]['Age']=pd.cut(dataset[i]['Age'],bins=bins ,labels=labels)\n    dataset[i]['Age']=dataset[i]['Age'].astype('int64')\nprint('training dataset:')\ndisplay(dataset[0].head())\nprint('testing dataset:')\ndisplay(dataset[1].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Normalizing our inout data in training and testing dataset\nX=dataset[0].iloc[:, 1:]\nY=dataset[0].iloc[:, 0]\nx_test=dataset[1].iloc[:, 0:]\nnormalized_data = X\nnormalized_data=normalized_data.append(x_test)\nnormalized_x_train = normalized_data.values\nnormalized_x_train /= np.max(np.abs(normalized_x_train),axis=0)\n\nX = pd.DataFrame(normalized_x_train[:891,:], \n                      columns=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title'])\nprint(X.head())\nprint(len(X))\nx_test = pd.DataFrame(normalized_x_train[891:,:], \n                      columns=['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title'])\ndisplay(x_test.head())\nprint(len(x_test))\ndisplay(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split our training set into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size = 0.20)\nprint(\"Training set shape: \"+str(X_train.shape))\nprint(\"Validation set shape: \"+str(X_val.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown\n## Machine learning algorithms for classification\n1) Logistic Regression <br>\n2) Decision Tree <br>\n3) Random Forest <br>\n4) XGBoost <br>","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"accuracies_list = list()\naccuracies = namedtuple('accuracies',('Model', 'accuracy'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown\n#### Logistic Regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_val)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\naccuracies_list.append(accuracies('Logistic Regression', acc_log))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown\n## Decision Tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"decisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\naccuracies_list.append(accuracies('Decision Tree', acc_decisiontree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown\n#### Random Forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nclf = RandomForestClassifier(max_depth=10, max_leaf_nodes =20,random_state=0)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_val)\nacc_random_forest = round(accuracy_score(y_pred, y_val) * 100, 2)\naccuracies_list.append(accuracies('Random Forest', acc_random_forest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown\n#### XGBoost","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb = xgboost.XGBClassifier(random_state=5,learning_rate=0.01)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\naccuracies_list.append(accuracies('XGBoost', acc_xgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown\n## **Deep learning for binary classification with pytorch**\n1) Declare consts <br>\n2) Training Set && Testing Set preparation for pytorch <br>\n3) Define our DL model class <br>\n4) Instantiate our model, loss and optimizer <br>\n5) Define fit function <br>\n6) Training process <br>\n7) Define Predict Function <br>\n8) Preprare for submission <br>","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Constants\nBATCH_SIZE = 1\nLEARNING_RATE = 0.001\nEPOCHS = 800\nINPUT_NODES = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"## Training Set && Testing Set preparation for pytorch \n\n#Create Tensors from our dataframes\nX_train_torch = torch.from_numpy(X_train.values).type(torch.FloatTensor) # Train X\ny_train_torch = torch.from_numpy(y_train.values).type(torch.LongTensor) # Train Y\nX_val_torch = torch.from_numpy(X_val.values).type(torch.FloatTensor) # Validate X\ny_val_torch = torch.from_numpy(y_val.values).type(torch.LongTensor) # Validate Y\nx_test_torch = torch.from_numpy(x_test.values).type(torch.FloatTensor) # Test X\n\n#Create Tensordatasets for pytorch\ntrain = torch.utils.data.TensorDataset(X_train_torch,y_train_torch) # Train\nval = torch.utils.data.TensorDataset(X_val_torch, y_val_torch) # Validate\ntest = torch.utils.data.TensorDataset(x_test_torch) # Test\n\n#Create data loaders\ndata_loader = torch.utils.data.DataLoader(train) # Train\nval_loader = torch.utils.data.DataLoader(val) # Validate\ntest_loader = torch.utils.data.DataLoader(test) # Test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%markdown \n### **Define model Class**\nInput Features --> Fully Connected layer(512 nodes) --> Dropout(50%) --> Fully Connected layer(256 nodes) --> Dropout(50%) --> Fully Connected layer(128 nodes) --> Dropout(50%) --> Fully Connected layer(1 node)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class Titanic_NN(nn.Module):\n    \"\"\"\n    Class for our Neural network to make best predictions ever ya jean in the titanic dataset\n    3 fully connected hidden Layers + output Layer with dropout of 0.5 probability each.\n    We have 8 input nodes and 1 output node (1 if >0.5 else 0)\n    NB: We didn't use batch normalization because we didn't use batches.\n    \"\"\"\n    def __init__(self, INPUT_NODES):\n        super(Titanic_NN, self).__init__()\n        self.fc1 = nn.Linear(INPUT_NODES,512)\n        #self.batcTitanic_NNh_norm1 = nn.BatchNorm1d(1)\n        self.fc2 = nn.Linear(512,256)\n        #self.batch_norm2 = nn.BatchNorm1d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128,1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        #x = self.batch_norm1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        #x = self.batch_norm2(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.fc4(x)\n        \n        return F.sigmoid(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Model - Loss - Optimizer\n# Instantiate our class Titanic_NN\nmodel = Titanic_NN(INPUT_NODES)\ntry: #if we have a trained model, we load it\n    model.load_state_dict(torch.load(input_path+'titanic_model_4layers'))\nexcept:\n    pass\n\n#Binary Classification Entropy Loss\nerror = nn.BCELoss()\n\n#SGD Optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\nprint(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def fit(model, data, phase='training', batch_size = 1, is_cuda=False, input_dim = 8):\n    \"\"\"\n    Method to train the model\n    :input model:(Titanic_NN) Our model (in this case is the Titanic_NN model class)\n    :input data:(dataLoader) our training / validation data to train the model on and validate\n    :input phase: (String) 'training' to train model 'validation' to make predictions and validate the model\n    :input batch_size: (int) batch size we feed to our neural network\n    :input is_cuda: (Bool) wheather to use cuda or not (GPU) (still not implemented)\n    :input input_dim: (int) number of input nodes\n    :return loss: (float) loss value for one epoch\n    :return accuracy: (float) accuracy value for one epoch\n    \"\"\"\n    if phase == 'training':\n        model.train()\n    elif phase == 'validation':\n        model.eval()\n    loss_values = 0.0\n    correct_values = 0\n    for batch_idx, (features, label) in enumerate(data):\n        if is_cuda:\n            features, label = features.cuda(), label.cuda()\n        features, label = Variable(features.view(batch_size, 1, input_dim)), Variable(label.float().view(-1, 1))\n        if phase == 'training':\n            optimizer.zero_grad() \n        output = model(features) #make one forward pass\n        loss = error(output, label) #calculate loss for one forxard pass\n        loss_values += loss.data\n        if output[0] > 0.5:\n            predictions = torch.Tensor([1])\n        else:\n            predictions = torch.Tensor([0])\n        correct_values += predictions.eq(label.data.view_as(predictions)).cpu().sum()\n        if phase == 'training':\n            loss.backward() #\n            optimizer.step()  # make one gradient step\n    loss = loss_values / len(data.dataset) # calculate loss mean for the epoch\n    accuracy = 100. * correct_values / len(data.dataset) #calculate accuracy\n    if phase == 'validation':\n        print(f'\\n{phase} loss is {loss:{5}.{2}} and {phase} accuracy is \\\n              {accuracy:{10}.{4}}\\n=============================================') #we can choose to print both phases\n    return loss, accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Training process\n#validation / traing lists for accuracy and loss values during training\ntrain_loss_list, val_loss_list = [], []\ntrain_accuracy_list, val_accuracy_list = [], []\n\nfor epoch in range(EPOCHS):\n    print(epoch)\n    #Training\n    train_epoch_loss, train_epoch_accuracy = fit(model, data_loader, batch_size=BATCH_SIZE, input_dim=INPUT_NODES) \n    #Validating\n    val_epoch_loss, val_epoch_accuracy = fit(model, val_loader, phase='validation', batch_size=BATCH_SIZE, input_dim=INPUT_NODES)\n    if epoch % 50 == 0:\n        torch.save(model.state_dict(), 'titanic_model_4layers')\n    train_loss_list.append(train_epoch_loss)\n    train_accuracy_list.append(train_epoch_accuracy)\n    val_loss_list.append(val_epoch_loss)\n    val_accuracy_list.append(val_epoch_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"accuracies_list.append(accuracies('Neural Network __Validation_Set__', val_accuracy_list[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def predict(model, data):\n    \"\"\"\n    Make predictions on the test dataset\n    :input model: (Titanic_NN) our model DL to make predictions with\n    :input data: (torch.utils.data.DataLoader) our test data loader\n    :output test_predictions: (list()) list of predictions\n    \"\"\"\n    model.eval()\n    test_predictions = list()\n    for batch_idx, (feature,) in enumerate(data):\n        feature = Variable(feature.view(1, 1, INPUT_NODES))\n        output = model(feature)\n        if output[0] > 0.5:\n            prediction = 1\n        else:\n            prediction = 0\n        test_predictions.append(prediction)\n    return test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#create our dataframe of predictions and indices from test_set\n#our_predictions = predict(model, test_loader)\npred_df = pd.DataFrame(np.c_[np.arange(892, len(test_set)+892)[:,None], predict(model, test_loader)], \n                      columns=['PassengerId', 'Survived'])\n#save results to a csv file\npred_df.to_csv('titanic_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(accuracies_list)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"titanic","language":"python","name":"titanic"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":4}